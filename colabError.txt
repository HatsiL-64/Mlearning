!pip install paddlepaddle-gpu==2.6.1 -U
!pip install "paddleocr>=2.7.0,<3.0.0" -U
!pip install langchain==0.1.0 langchain-community==0.0.10 gTTS pyngrok --quiet
!pip install ultralytics
!pip install flask pyngrok --quiet

import os
import webbrowser
import sys
from flask import Flask, request, render_template_string, send_file, jsonify
from paddleocr import PaddleOCR
import re
from gtts import gTTS
import cv2
import numpy as np
from ultralytics import YOLO
from pyngrok import ngrok
from google.colab.patches import cv2_imshow
from google.colab import output
from IPython.display import display, HTML
import requests

#Configurar ngrok
Token="3AMgsXXyqJVygtTOEVMFjczHRsz_4vzUeX29b4P9Z4zGbD99z"
#Token="39XWnEGQ4sFO2zBwbTi4GC0QCli_2DbKSuwte8EKb2qc3bL7g"
#Token="3AA7OHVpWiU2zPYE2H3c"
ngrok.set_auth_token(Token)
app=Flask(__name__)

#Inicializar Paddle
ocr=PaddleOCR(use_angle_cls=True,lang='es',ocr_version='PP-OCRv4',show_log=False)

#Inicializar Yolo
yolo = YOLO('yolov8n.pt')

#Dice el texto detectado
def Visibilidad(gris):
    if len(gris.shape)==3:
        gris=cv2.cvtColor(gris,cv2.COLOR_BGR2GRAY)
    else:
        gris=gris
    agrandar=cv2.resize(gris,None,fx=2.5,fy=2.5,interpolation=cv2.INTER_LANCZOS4)
    clahe=cv2.createCLAHE(clipLimit=3.0,tileGridSize=(8,8))
    mejorada=clahe.apply(agrandar)
    #Enfocar
    enfoque=np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])
    enfocado=cv2.filter2D(mejorada,-1,enfoque)
    #Limpia puntos
    final=cv2.fastNlMeansDenoising(enfocado,None,10,7,21)
    return final

def hablar(texto):
    if not texto.strip():
        return
    tts=gTTS(text=texto, lang='es')
    tts.save("voz.mp3")

@app.route('/')
def interfaz():
    return render_template_string('''
      <!DOCTYPE html>
      <html>
      <head>
        <meta charset="UTF-8">
        <title>Audio</title>
        <style>
          #btn
          {
            width: 100%;
            height: 98vh;
            background-color: #2c3e50;
            cursor: pointer;
            border: none;
          }
        </style>
      </head>
      <body>
        <button id="btn"></button>
        <audio id="audio"></audio>
        <script>
          const boton=document.getElementById("btn");
          const audio=document.getElementById("audio");
          let ult=0;
          boton.onclick = () => {
                if (audio.paused)
                  audio.play();
                else
                  audio.pause();
          };
          //Revisa 3 segundos si hay audio nuevi
          setInterval(async ()=>{
              if(!audio.ended && audio.currentTime>0)
                return;
              try
              {
                //Para saber cuando creó el audio
                const res = await fetch('/audio');
                const dat=await res.json();
                if(dat.timestamp > ult)
                {
                  ult=dat.timestamp;
                  audio.src="/descarga?v="+Date.now();
                  audio.play();
                }
              }
              catch(e)
              {
                console.log("esperando");
              }
          },3000);
        </script>
      </body>
      </html>
    ''')
@app.route('/detectar', methods=['POST'])
def detectar():
    try:
        img=request.data
        if not img:
            return "No hay datos",400
        #Alista la imagen para trabajarla con yolo
        imagenCv = np.frombuffer(img,np.uint8)
        img=cv2.imdecode(imagenCv,cv2.IMREAD_COLOR)
        resultados = yolo(img)

        #crea un arreglo donde guardara los objetos que identifique
        objetos_detectados = []
        for result in resultados:
            for box in result.boxes:
                # obtiene la clase del objeto y la confianza
                clase_id = int(box.cls[0])
                confianza = float(box.conf[0])
                nombre_objeto = yolo.names[clase_id]

                #filtra por confianza y por objetos considerados obstaculos
                if confianza > 0.5:  #umbral de confianza
                    #coordenadas del objeto
                    x1, y1, x2, y2 = map(int, box.xyxy[0])

                    #guarda cada bjeto en un arreglo, cada objeto es un diccionario
                    objetos_detectados.append({
                        'objeto': nombre_objeto,
                        'confianza': round(confianza, 2),
                        'posicion': (x1, y1, x2, y2)
                    })

                    #dibuja el cuadro en la imagen
                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(img, f"{nombre_objeto} ({confianza:.2f})",
                              (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        #menciona los objetos detectados
        if objetos_detectados:
            print("objetos detectados:")
            for obs in objetos_detectados:
                print(f"  - {obs['objeto']} (confianza: {obs['confianza']})")

        else:
            print("Camino despejado.")

    except Exception as e:
        print(f"Error: {e}")
        return "Hay error"

@app.route('/audio')
def audio():
  if os.path.exists("voz.mp3"):
      #Cuando se creo el archivo
      tiempo=os.path.getmtime("voz.mp3")
      return jsonify({"timestamp":tiempo})
  return jsonify({"timestamp":0})

@app.route('/descarga')
def descarga():
    return send_file("voz.mp3")

@app.route('/procesar',methods=['POST'])
def procesar():
    try:
        imagenRe=request.data
        if not imagenRe:
            return "No hay datos",400
        imagenCv = np.frombuffer(imagenRe,np.uint8)
        imag=cv2.imdecode(imagenCv,cv2.IMREAD_COLOR)
        imagFinal=Visibilidad(imag)
        cv2_imshow(imagFinal)

        resultado=ocr.ocr(imagFinal,cls=True)
        textoCompleto=""
        if resultado and resultado[0]:
            for linea in resultado[0]:
                textoDetectado =linea[1][0]
                mejor=linea[1][1]
                if mejor > 0.45:
                    textoCompleto+=textoDetectado+" "
        textoFinal=re.sub(r'\s+', ' ',textoCompleto).strip()

        print(f"Texto:{textoFinal}")

        #Función hablar
        if textoFinal:
            hablar(textoFinal)
            return textoFinal
        else:
            return "No se detecto bien",200

    except Exception as e:
        print(f"Error: {e}")
        return "Hay error"

try:
    r = requests.get("http://127.0.0.1:5000")
    print("Respuesta local:", r.text)
except Exception as e:
    print("Flask no está corriendo:", e)


if __name__ == '__main__':
    #!pkill ngrok
    ngrok.kill()
    try:
        ngrok.set_auth_token(Token)
        # Abrir el túnel
        tunel = ngrok.connect(5000)
        urlP=tunel.public_url

        # Abrir automáticamente y mostrar link de respaldo
        display(HTML(f'<h2><a href="{urlP}" target="_blank">Click aquí si no abre solo</a></h2>'))
        output.eval_js(f'window.open("{urlP}", "_blank")')

        print( f"Link:{urlP}")
        # 3. Correr Flask
        app.run(port=5000,host="0.0.0.0",threaded=True,debug=False,use_reloader=False)
    except Exception as e:
        print(f"Error al iniciar ngrok: {e}")