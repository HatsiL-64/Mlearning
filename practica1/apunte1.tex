\documentclass{article}

%los comentarios se ponen con %

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish,provide=*]{babel}
\usepackage{amsmath}

\title{Machine learning y redes neuronales}
\date{}

\begin{document}

\maketitle
\section{MCP}

Con el fin de dise;ar una IA, se definio a una celula nerviosa como una puerta logica con salidas binarias. A esta celula nerviosa entran multiples datos por las dendritas, estos datos se agrupan y si la se;al acumulada supera
cierto umbral, se genera una salida transmitida por el axon. A la abstraccion matematica de este modelo de neurona se le llama MCP.

Una neurona artificial es usada para tareas de clasificacion binaria, donde existen dos clases, la primera clase, clase positiva(1) y la segunda, clase negativa(-1).\\
Una funcion de desicion \(\phi(Z)\) es aquella que determina la clase a la que pertenece cada muestra. Z esta dada por una combinacion lineal de ciertos valores de entrada(X) y sus pesos correspondientes(W).                                                                   
\[
  Z = \sum_{k=0}^{N} X_k W_k 
\]
Es decir que \(Z\) es el producto punto entre los vectores \(X\) y \(W\).
  \[ 
  Z = X \cdot W
  \]
Si \(\phi(Z)\) es mayor que un umbral definido \(\theta\) en una muestra determinada, se dice que esta muestra pertenece a la clase 1. Para fines practicos podemos igualar a \(X_0 = 1\) y \(W_0 = -\theta\).
Un umbral negativo tambien se le puede llamar cesgo.

Si \(\phi(Z) > \theta\) entonces la muestra pertenece a la clase positiva.
Si \(\phi(Z) \leq \theta\) entonces la muestra pertenece a la clase negativa.

\section{Aprendizaje del Perceptron}

El perceptron es el tipo mas simple de red neuronal. El perceptron umbralizado y el MCP imitan el funcionamiento de una neurona biologica. Estos modelos son capaces de clasificar muestras dentro de dos clases siguiendo la
\textbf{Regla del perceptron inicial de Rosenblatt}, la cual se reduce a dos pasos:

\begin{enumerate}
  \item Iniciar los pesos a numeros aleatorios mayores pero cercanos a 0 
  \item Por cada muestra en el entrenamiento \(X^i\) se calcula el valor \(\hat{y}\) y se actualizan los pesos \(W\)
\end{enumerate}

Para actualizar los pesos usamos la formula:
\[
  W_j := W_j + \Delta W_j
\]
Para obtener \(\Delta W_j\) usamos la siguiente regla:
\[
  \Delta W_j = \eta(y^i - \hat{y}^i)X^i_j
\]
  Donde \(\eta =\) rango de aprendizaje, \(y^i =\) etiqueta de clase de la muestra y \(\hat{y} =\) etiqueta de clase predicha.

Algo importante a destacar es que el perceptron tiene mayor precision si las dos clases son linealmente separables y si el rango de aprendizaje es peque;o. Si las dos clases no son linealmente separables podemos ajustar
un limite de correcciones a los pesos y un umbral para el numero de clasificaciones erroneas. Si no se hace esto, el perceptron seguira ajustando los pesos infinitamente
  
\end{document}
